{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {"id": "header"},
      "source": ["# Система обработки резюме и анализа данных о вакансиях\n", "\n", "**Архитектура**: Паттерн Chain of Responsibility для модульной предобработки данных\n", "\n", "**Назначение**: Загрузка CSV, трансформация признаков, подготовка матриц признаков (X) и целевой переменной (y) в формате NumPy"]
    },
    {
      "cell_type": "markdown",
      "metadata": {"id": "imports_section"},
      "source": ["## Инициализация окружения и импорты"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {"id": "imports"},
      "outputs": [],
      "source": ["from __future__ import annotations\n", "\n", "import argparse\n", "import logging\n", "import re\n", "import sys\n", "from abc import ABC, abstractmethod\n", "from pathlib import Path\n", "from typing import Optional, Sequence, Tuple, List, Dict, Any, Set\n", "\n", "import numpy as np\n", "import pandas as pd\n", "\n", "logging.basicConfig(\n", "    level=logging.INFO,\n", "    format='%(asctime)s [%(levelname)s] %(name)s: %(message)s',\n", "    datefmt='%Y-%m-%d %H:%M:%S'\n", ")\n", "logger = logging.getLogger(__name__)"]
    },
    {
      "cell_type": "markdown",
      "metadata": {"id": "chain_section"},
      "source": ["## Ядро архитектуры: цепочка обработчиков"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {"id": "base_processor"},
      "outputs": [],
      "source": ["class ProcessorStep(ABC):\n", "    \"\"\"Абстрактный обработчик в цепочке ответственности.\n", "    \n", "    Каждый обработчик имеет уникальную ответственность и может передать\n", "    результат следующему обработчику в цепочке.\n", "    \"\"\"\n", "\n", "    def __init__(self) -> None:\n", "        self._successor: Optional[ProcessorStep] = None\n", "\n", "    def set_next(self, handler: ProcessorStep) -> ProcessorStep:\n", "        \"\"\"Установить следующий обработчик в цепочке.\"\"\"\n", "        self._successor = handler\n", "        return handler\n", "\n", "    def handle(self, data: Optional[pd.DataFrame]) -> Optional[pd.DataFrame]:\n", "        \"\"\"Обработать данные и передать результат дальше.\"\"\"\n", "        if data is None:\n", "            logger.warning(f'{self.__class__.__name__}: получены None данные')\n", "            return None\n", "\n", "        logger.info(f'{self.__class__.__name__}: обработка {data.shape[0]} строк, {data.shape[1]} колонок')\n", "        \n", "        processed = self.process(data)\n", "        \n", "        if self._successor is not None:\n", "            return self._successor.handle(processed)\n", "        return processed\n", "\n", "    @abstractmethod\n", "    def process(self, data: pd.DataFrame) -> pd.DataFrame:\n", "        \"\"\"Реализовать логику обработки.\"\"\"\n", "        raise NotImplementedError"]
    },
    {
      "cell_type": "markdown",
      "metadata": {"id": "processors"},
      "source": ["## Конкретные обработчики данных"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {"id": "processor_loader"},
      "outputs": [],
      "source": ["class DataSourceLoader(ProcessorStep):\n", "    \"\"\"Загружает CSV файл с автоматическим определением кодировки.\"\"\"\n", "\n", "    SUPPORTED_ENCODINGS: Sequence[str] = ('utf-8', 'cp1251', 'iso-8859-5')\n", "\n", "    def __init__(self, filepath: Path | str, **csv_options) -> None:\n", "        super().__init__()\n", "        self.filepath = Path(filepath)\n", "        self.csv_options = csv_options\n", "\n", "    def process(self, _: Optional[pd.DataFrame]) -> pd.DataFrame:\n", "        if not self.filepath.exists():\n", "            raise FileNotFoundError(f'CSV не найден: {self.filepath}')\n", "\n", "        for encoding in self.SUPPORTED_ENCODINGS:\n", "            try:\n", "                logger.debug(f'Попытка загрузить с кодировкой: {encoding}')\n", "                df = pd.read_csv(\n", "                    self.filepath,\n", "                    sep=',',\n", "                    quotechar='\"',\n", "                    engine='python',\n", "                    encoding=encoding,\n", "                    index_col=0,\n", "                    **self.csv_options\n", "                )\n", "                df.columns = [str(c).strip() for c in df.columns]\n", "                logger.info(f'✓ Загрузка успешна с {encoding}')\n", "                return df\n", "            except (UnicodeDecodeError, pd.errors.ParserError):\n", "                continue\n", "\n", "        raise RuntimeError(f'Не удалось загрузить CSV ни с одной из кодировок')"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {"id": "processor_sanitizer"},
      "outputs": [],
      "source": ["class TextFieldSanitizer(ProcessorStep):\n", "    \"\"\"Очистка текстовых полей от нежелательных символов.\"\"\"\n", "\n", "    @staticmethod\n", "    def _sanitize(value: Any) -> Any:\n", "        if not isinstance(value, str):\n", "            return value\n", "        \n", "        s = value.replace('\\\\ufeff', '').replace('\\\\xa0', ' ')\n", "        s = re.sub(r'[\\\\t\\\\n\\\\r]+', ' ', s)\n", "        s = ''.join(ch for ch in s if ch.isprintable() or ch == ' ')\n", "        s = re.sub(r'\\\\s+', ' ', s)\n", "        return s.strip()\n", "\n", "    def process(self, data: pd.DataFrame) -> pd.DataFrame:\n", "        result = data.copy()\n", "        text_cols = result.select_dtypes(include=['object', 'string']).columns\n", "        \n", "        for col in text_cols:\n", "            result[col] = result[col].apply(self._sanitize)\n", "            \n", "        return result"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {"id": "processor_salary"},
      "outputs": [],
      "source": ["class SalaryNormalizer(ProcessorStep):\n", "    \"\"\"Преобразование зарплаты в единую валюту (рубли).\"\"\"\n", "\n", "    CURRENCY_RATES: Dict[str, float] = {\n", "        'rub': 1.0, 'руб': 1.0, 'руб.': 1.0,\n", "        'usd': 78.5, 'eur': 91.0,\n", "        'kzt': 0.152, 'uah': 1.8,\n", "        'gbp': 105.0, 'cny': 11.2,\n", "    }\n", "\n", "    def __init__(self, salary_column: str = 'ЗП') -> None:\n", "        super().__init__()\n", "        self.salary_column = salary_column\n", "\n", "    def process(self, data: pd.DataFrame) -> pd.DataFrame:\n", "        if self.salary_column not in data.columns:\n", "            return data\n", "\n", "        result = data.copy()\n", "        numbers = result[self.salary_column].astype(str).str.extract(r'(\\\\d+[.,]?\\\\d*)', expand=False)\n", "        numbers = pd.to_numeric(numbers.str.replace(',', '.'), errors='coerce')\n", "        \n", "        currencies = result[self.salary_column].astype(str).str.extract(r'([A-Za-zА-Яа-яёЁ.]+)', expand=False).fillna('')\n", "        currencies = currencies.str.lower().str.rstrip('.')\n", "        \n", "        def convert_to_rub(amount: float, currency: str) -> Optional[float]:\n", "            if pd.isna(amount):\n", "                return None\n", "            rate = self.CURRENCY_RATES.get(currency, 1.0)\n", "            return amount * rate\n", "        \n", "        result[self.salary_column] = [convert_to_rub(a, c) for a, c in zip(numbers, currencies)]\n", "        return result"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {"id": "processor_outliers"},
      "outputs": [],
      "source": ["class OutlierProcessor(ProcessorStep):\n", "    \"\"\"Обработка выбросов в целевой переменной через IQR метод.\"\"\"\n", "\n", "    def __init__(self, target_col: str = 'ЗП', multiplier: float = 1.5, mode: str = 'clip') -> None:\n", "        super().__init__()\n", "        self.target_col = target_col\n", "        self.multiplier = multiplier\n", "        self.mode = mode\n", "\n", "    def process(self, data: pd.DataFrame) -> pd.DataFrame:\n", "        if self.target_col not in data.columns:\n", "            return data\n", "\n", "        result = data.copy()\n", "        target = result[self.target_col].dropna()\n", "        \n", "        q1 = target.quantile(0.25)\n", "        q3 = target.quantile(0.75)\n", "        iqr_range = q3 - q1\n", "        \n", "        lower_bound = q1 - self.multiplier * iqr_range\n", "        upper_bound = q3 + self.multiplier * iqr_range\n", "        \n", "        outlier_mask = (result[self.target_col] < lower_bound) | (result[self.target_col] > upper_bound)\n", "        \n", "        if self.mode == 'clip':\n", "            result[self.target_col] = result[self.target_col].clip(lower=lower_bound, upper=upper_bound)\n", "        elif self.mode == 'remove':\n", "            result = result.loc[~outlier_mask].reset_index(drop=True)\n", "        \n", "        return result"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {"id": "processor_quality"},
      "outputs": [],
      "source": ["class DataQualityEnforcer(ProcessorStep):\n", "    \"\"\"Работа с полнотой и качеством данных.\"\"\"\n", "\n", "    def __init__(self, remove_dups: bool = True, nan_threshold: float = 0.5) -> None:\n", "        super().__init__()\n", "        self.remove_dups = remove_dups\n", "        self.nan_threshold = nan_threshold\n", "\n", "    def process(self, data: pd.DataFrame) -> pd.DataFrame:\n", "        result = data.copy()\n", "        \n", "        if self.remove_dups and len(result) > 50:\n", "            result = result.drop_duplicates()\n", "        \n", "        threshold = int(self.nan_threshold * len(result))\n", "        result = result.dropna(axis=1, thresh=threshold)\n", "        \n", "        numeric_cols = result.select_dtypes(include=['number']).columns\n", "        for col in numeric_cols:\n", "            result[col] = result[col].fillna(result[col].median())\n", "        \n", "        categorical_cols = result.select_dtypes(include=['object', 'category']).columns\n", "        for col in categorical_cols:\n", "            result[col] = result[col].fillna('__undefined__')\n", "        \n", "        return result"]
    },
    {
      "cell_type": "markdown",
      "metadata": {"id": "feature_section"},
      "source": ["## Главный энкодер признаков"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {"id": "feature_encoder"},
      "outputs": [],
      "source": ["class FeatureEngineer(ProcessorStep):\n", "    \"\"\"Комплексный энкодер признаков с множеством трансформаций.\"\"\"\n", "\n", "    POSITION_GROUPS: Dict[str, List[str]] = {\n", "        'development': ['программист', 'разработчик', 'developer', 'java', 'python'],\n", "        'infrastructure': ['системный', 'администратор', 'devops'],\n", "        'management': ['менеджер', 'руководитель', 'project manager'],\n", "        'analytics': ['аналитик', 'data', 'bi'],\n", "        'customer_support': ['поддержка', 'support'],\n", "        'other': []\n", "    }\n", "\n", "    MAJOR_CITIES: Set[str] = {'москва', 'мск', 'московская область'}\n", "    SPB_CITIES: Set[str] = {'санкт-петербург', 'спб'}\n", "    LARGE_CITIES: Set[str] = {'новосибирск', 'екатеринбург', 'казань', 'нижний новгород'}\n", "\n", "    @staticmethod\n", "    def _extract_gender(text: str) -> Optional[int]:\n", "        if not isinstance(text, str):\n", "            return None\n", "        t = text.lower()\n", "        if 'муж' in t or 'male' in t:\n", "            return 1\n", "        if 'жен' in t or 'female' in t:\n", "            return 0\n", "        return None\n", "\n", "    @staticmethod\n", "    def _extract_age(text: str) -> Optional[float]:\n", "        if not isinstance(text, str):\n", "            return None\n", "        m = re.search(r'(\\\\d{1,3})\\\\s*(?:лет|год|г|years?)', text.lower())\n", "        if m:\n", "            age = float(m.group(1))\n", "            return age if 15 <= age <= 80 else None\n", "        return None\n", "\n", "    @classmethod\n", "    def _classify_position(cls, title: str) -> str:\n", "        if not isinstance(title, str):\n", "            return 'other'\n", "        t = title.lower()\n", "        for group, keywords in cls.POSITION_GROUPS.items():\n", "            if any(kw in t for kw in keywords):\n", "                return group\n", "        return 'other'\n", "\n", "    @classmethod\n", "    def _classify_city(cls, city_name: str) -> str:\n", "        if not isinstance(city_name, str):\n", "            return 'small_city'\n", "        c = city_name.lower().split(',')[0].strip()\n", "        if any(k in c for k in cls.MAJOR_CITIES):\n", "            return 'moscow_region'\n", "        if any(k in c for k in cls.SPB_CITIES):\n", "            return 'spb_region'\n", "        if c in cls.LARGE_CITIES:\n", "            return 'large_city'\n", "        return 'small_city'\n", "\n", "    @staticmethod\n", "    def _extract_experience(text: str) -> Optional[float]:\n", "        if not isinstance(text, str) or 'не указано' in text.lower():\n", "            return None\n", "        m = re.search(r'(\\\\d+)\\\\s*(?:лет|год|г|years?)', text.lower())\n", "        if m:\n", "            return float(m.group(1))\n", "        return None\n", "\n", "    def _transform_demographics(self, df: pd.DataFrame) -> pd.DataFrame:\n", "        src_col = 'Пол, возраст'\n", "        if src_col not in df.columns:\n", "            return df\n", "        out = df.copy()\n", "        out['sex'] = out[src_col].apply(self._extract_gender).fillna(-1)\n", "        out['age_years'] = out[src_col].apply(self._extract_age)\n", "        out['age_years'] = out['age_years'].fillna(out['age_years'].median()).clip(18, 75)\n", "        out.drop(columns=[src_col], inplace=True, errors='ignore')\n", "        return out\n", "\n", "    def _transform_position_target(self, df: pd.DataFrame) -> pd.DataFrame:\n", "        src_col = 'Ищет работу на должность:'\n", "        if src_col not in df.columns:\n", "            return df\n", "        out = df.copy()\n", "        out['position_group'] = out[src_col].apply(self._classify_position)\n", "        out = pd.get_dummies(out, columns=['position_group'], prefix='pos', drop_first=True)\n", "        out.drop(columns=[src_col], inplace=True, errors='ignore')\n", "        return out\n", "\n", "    def _transform_location(self, df: pd.DataFrame) -> pd.DataFrame:\n", "        src_col = 'Город'\n", "        if src_col not in df.columns:\n", "            return df\n", "        out = df.copy()\n", "        out['city_cluster'] = out[src_col].apply(self._classify_city)\n", "        out = pd.get_dummies(out, columns=['city_cluster'], drop_first=True)\n", "        out.drop(columns=[src_col], inplace=True, errors='ignore')\n", "        return out\n", "\n", "    def _transform_employment(self, df: pd.DataFrame) -> pd.DataFrame:\n", "        src_col = 'Занятость'\n", "        if src_col not in df.columns:\n", "            return df\n", "        out = df.copy()\n", "        out['full_time'] = out[src_col].apply(lambda x: 1 if isinstance(x, str) and 'полная' in x.lower() else 0)\n", "        out['part_time'] = out[src_col].apply(lambda x: 1 if isinstance(x, str) and 'частична' in x.lower() else 0)\n", "        out.drop(columns=[src_col], inplace=True, errors='ignore')\n", "        return out\n", "\n", "    def _transform_experience(self, df: pd.DataFrame) -> pd.DataFrame:\n", "        src_col = 'Опыт (двойное нажатие для полной версии)'\n", "        if src_col not in df.columns:\n", "            return df\n", "        out = df.copy()\n", "        out['work_years'] = out[src_col].apply(self._extract_experience)\n", "        out['work_years'] = out['work_years'].fillna(out['work_years'].median()).clip(0, 50)\n", "        out.drop(columns=[src_col], inplace=True, errors='ignore')\n", "        return out\n", "\n", "    def process(self, data: pd.DataFrame) -> pd.DataFrame:\n", "        df = data.copy()\n", "        df = self._transform_demographics(df)\n", "        df = self._transform_position_target(df)\n", "        df = self._transform_location(df)\n", "        df = self._transform_employment(df)\n", "        df = self._transform_experience(df)\n", "        return df"]
    },
    {
      "cell_type": "markdown",
      "metadata": {"id": "export_section"},
      "source": ["## Экспорт и сохранение результатов"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {"id": "dataset_splitter"},
      "outputs": [],
      "source": ["class DatasetSplitter(ProcessorStep):\n", "    \"\"\"Разделение DataFrame на признаки (X) и целевую переменную (y).\"\"\"\n", "\n", "    KNOWN_TARGETS: List[str] = ['ЗП', 'Зарплата', 'salary', 'target', 'y']\n", "\n", "    def __init__(self, target_name: Optional[str] = None) -> None:\n", "        super().__init__()\n", "        self.target_name = target_name\n", "\n", "    def process(self, data: pd.DataFrame) -> pd.DataFrame:\n", "        return data\n", "\n", "    def split(self, data: pd.DataFrame) -> Tuple[pd.DataFrame, pd.Series]:\n", "        target_col = self._find_target_column(data)\n", "        X = data.drop(columns=[target_col])\n", "        y = data[target_col]\n", "        logger.info(f'Разделение: X shape={X.shape}, y shape={y.shape}')\n", "        return X, y\n", "\n", "    def _find_target_column(self, data: pd.DataFrame) -> str:\n", "        if self.target_name is not None:\n", "            return self.target_name\n", "        for name in self.KNOWN_TARGETS:\n", "            if name in data.columns:\n", "                logger.info(f'Целевая колонка: {name}')\n", "                return name\n", "        return data.columns[-1]"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {"id": "numpy_persister"},
      "outputs": [],
      "source": ["class NumpyPersister(ProcessorStep):\n", "    \"\"\"Сохраняет обработанные данные в формате NumPy.\"\"\"\n", "\n", "    def __init__(self, source_path: Path | str, features_file: str = 'X.npy', target_file: str = 'y.npy', target_name: Optional[str] = None) -> None:\n", "        super().__init__()\n", "        self.output_dir = Path(source_path).parent\n", "        self.features_file = features_file\n", "        self.target_file = target_file\n", "        self.splitter = DatasetSplitter(target_name)\n", "\n", "    def process(self, data: pd.DataFrame) -> pd.DataFrame:\n", "        X, y = self.splitter.split(data)\n", "        \n", "        X_path = self.output_dir / self.features_file\n", "        y_path = self.output_dir / self.target_file\n", "        \n", "        X_array = X.astype(float).to_numpy()\n", "        y_array = y.astype(float).to_numpy()\n", "        \n", "        np.save(X_path, X_array)\n", "        np.save(y_path, y_array)\n", "        \n", "        logger.info(f'✓ Сохранено: {X_path} {X_array.shape}')\n", "        logger.info(f'✓ Сохранено: {y_path} {y_array.shape}')\n", "        \n", "        return data"]
    },
    {
      "cell_type": "markdown",
      "metadata": {"id": "orchestration"},
      "source": ["## Оркестрация пайплайна"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {"id": "pipeline_assembly"},
      "outputs": [],
      "source": ["def assemble_pipeline(csv_path: Path, target_col: Optional[str] = None) -> ProcessorStep:\n", "    \"\"\"Собрать полный пайплайн из отдельных обработчиков.\"\"\"\n", "    \n", "    loader = DataSourceLoader(csv_path)\n", "    sanitizer = TextFieldSanitizer()\n", "    salary_norm = SalaryNormalizer(salary_column='ЗП')\n", "    outlier_proc = OutlierProcessor(target_col='ЗП')\n", "    quality_proc = DataQualityEnforcer()\n", "    features = FeatureEngineer()\n", "    persister = NumpyPersister(csv_path, target_name=target_col)\n", "    \n", "    loader.set_next(sanitizer).set_next(salary_norm).set_next(outlier_proc).set_next(quality_proc).set_next(features).set_next(persister)\n", "    \n", "    logger.info('Пайплайн собран')\n", "    return loader\n", "\n", "\n", "def execute_pipeline(csv_path: Path, target_col: Optional[str] = None) -> pd.DataFrame:\n", "    \"\"\"Выполнить обработку CSV.\"\"\"\n", "    loader = assemble_pipeline(csv_path, target_col)\n", "    result = loader.handle(None)\n", "    return result"]
    },
    {
      "cell_type": "markdown",
      "metadata": {"id": "main_section"},
      "source": ["## Точка входа"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {"id": "main_func"},
      "outputs": [],
      "source": ["def main(args: Optional[List[str]] = None) -> None:\n", "    \"\"\"Главная функция обработки данных.\n", "    \n", "    Использование: python app.py path/to/hh.csv\n", "    \"\"\"\n", "    parser = argparse.ArgumentParser(description='Обработка данных вакансий')\n", "    parser.add_argument('input_file', type=str, help='Путь к CSV файлу')\n", "    parser.add_argument('--target', type=str, default=None, help='Целевая колонка')\n", "    \n", "    parsed = parser.parse_args(args)\n", "    input_path = Path(parsed.input_file)\n", "    \n", "    if not input_path.exists():\n", "        logger.error(f'Файл не найден: {input_path}')\n", "        sys.exit(1)\n", "    \n", "    if input_path.suffix.lower() != '.csv':\n", "        logger.error(f'Неподдерживаемый формат: {input_path.suffix}')\n", "        sys.exit(1)\n", "    \n", "    logger.info(f'Начало обработки: {input_path}')\n", "    \n", "    try:\n", "        df_processed = execute_pipeline(input_path, parsed.target)\n", "        logger.info(f'✓ Обработка завершена! Форма: {df_processed.shape}')\n", "    except Exception as e:\n", "        logger.error(f'Ошибка: {e}', exc_info=True)\n", "        sys.exit(1)\n", "\n", "\n", "if __name__ == '__main__':\n", "    main()"]
    },
    {
      "cell_type": "markdown",
      "metadata": {"id": "demo"},
      "source": ["## Демонстрация"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {"id": "demo_run"},
      "outputs": [],
      "source": ["input_file = Path('hh.csv')\n", "target_column = 'ЗП'\n", "\n", "if input_file.exists():\n", "    logger.info(f'Начинаем обработку {input_file}')\n", "    processed_data = execute_pipeline(input_file, target_column)\n", "    logger.info('✓ Обработка завершена!')\n", "    \n", "    x_path = input_file.parent / 'X.npy'\n", "    y_path = input_file.parent / 'y.npy'\n", "    \n", "    if x_path.exists() and y_path.exists():\n", "        x_loaded = np.load(x_path)\n", "        y_loaded = np.load(y_path)\n", "        logger.info(f'X shape: {x_loaded.shape}')\n", "        logger.info(f'y shape: {y_loaded.shape}')\n", "else:\n", "    logger.warning(f'Файл {input_file} не найден')"]
    }
  ]
}
